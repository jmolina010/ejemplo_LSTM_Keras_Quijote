{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOiX2I9VitRs"
      },
      "source": [
        "# Ejemplo de red recurrente LSTM\n",
        "\n",
        "## Implementada con Keras y que genera texto, aprendiendo anivel de caracter teniendo como base de entrenamiento el libro \"Don Quijote de la Mancha\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E7Rv7Zp4jIt2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdJPszLqioKk",
        "outputId": "73cc7643-c826-4efd-f176-efa97e33ab71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "2226045/2226045 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/Don-Quijote.txt\n",
            "Longitud del dataset: 2168460 caracteres.\n",
            "Tamaño del vocabulario: 106 caracteres.\n"
          ]
        }
      ],
      "source": [
        "file = 'https://www.gutenberg.org/files/2000/2000-0.txt' # link a la versión txt UTF-8 del libro en la web del proyecto Gutemberg\n",
        "got_file = tf.keras.utils.get_file('Don-Quijote.txt', file)\n",
        "\n",
        "print(got_file) # imprime la ruta al fichero descargado\n",
        "\n",
        "texto = open(got_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Longitud del dataset: {len(texto)} caracteres.')\n",
        "\n",
        "# al convertir a conjunto, se eliminan caracteres duplicados, así que en\n",
        "# vocabulario se guarda un ejemplar de cada caracter de los que se encuentran\n",
        "# en el texto\n",
        "vocabulario = sorted(set(texto))\n",
        "print(f'Tamaño del vocabulario: {len(vocabulario)} caracteres.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT8z2TPnkzq_",
        "outputId": "d5d032fa-7315-49d1-88ca-7950d97e24f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '?': 28, '@': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '[': 56, ']': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '¡': 84, '«': 85, '»': 86, '¿': 87, 'Á': 88, 'É': 89, 'Í': 90, 'Ñ': 91, 'Ó': 92, 'Ú': 93, 'à': 94, 'á': 95, 'é': 96, 'í': 97, 'ï': 98, 'ñ': 99, 'ó': 100, 'ù': 101, 'ú': 102, 'ü': 103, '—': 104, '\\ufeff': 105}\n",
            "{0: '\\n', 1: '\\r', 2: ' ', 3: '!', 4: '\"', 5: '#', 6: '$', 7: '%', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: '@', 30: 'A', 31: 'B', 32: 'C', 33: 'D', 34: 'E', 35: 'F', 36: 'G', 37: 'H', 38: 'I', 39: 'J', 40: 'K', 41: 'L', 42: 'M', 43: 'N', 44: 'O', 45: 'P', 46: 'Q', 47: 'R', 48: 'S', 49: 'T', 50: 'U', 51: 'V', 52: 'W', 53: 'X', 54: 'Y', 55: 'Z', 56: '[', 57: ']', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '¡', 85: '«', 86: '»', 87: '¿', 88: 'Á', 89: 'É', 90: 'Í', 91: 'Ñ', 92: 'Ó', 93: 'Ú', 94: 'à', 95: 'á', 96: 'é', 97: 'í', 98: 'ï', 99: 'ñ', 100: 'ó', 101: 'ù', 102: 'ú', 103: 'ü', 104: '—', 105: '\\ufeff'}\n",
            "Primeros 100 caracteres del libro codificados:\n",
            " [105, 49, 65, 62, 2, 45, 75, 72, 67, 62, 60, 77, 2, 36, 78, 77, 62, 71, 59, 62, 75, 64, 2, 62, 31, 72, 72, 68, 2, 72, 63, 2, 33, 72, 71, 2, 46, 78, 66, 67, 72, 77, 62, 12, 2, 59, 82, 2, 42, 66, 64, 78, 62, 69, 2, 61, 62, 2, 32, 62, 75, 79, 58, 71, 77, 62, 76, 2, 48, 58, 58, 79, 62, 61, 75, 58, 1, 0, 1, 0, 49, 65, 66, 76, 2, 62, 31, 72, 72, 68, 2, 66, 76, 2, 63, 72, 75, 2, 77, 65]\n",
            "Primeros 100 caracteres del libro DEcodificados:\n",
            " ['\\ufeff', 'T', 'h', 'e', ' ', 'P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'D', 'o', 'n', ' ', 'Q', 'u', 'i', 'j', 'o', 't', 'e', ',', ' ', 'b', 'y', ' ', 'M', 'i', 'g', 'u', 'e', 'l', ' ', 'd', 'e', ' ', 'C', 'e', 'r', 'v', 'a', 'n', 't', 'e', 's', ' ', 'S', 'a', 'a', 'v', 'e', 'd', 'r', 'a', '\\r', '\\n', '\\r', '\\n', 'T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h']\n"
          ]
        }
      ],
      "source": [
        "# creamos los diccionarios que codifican cada caracter a número y cada número a caracter...\n",
        "char_index = {u:i for i,u in enumerate(vocabulario)}\n",
        "index_char = {i:u for i,u in enumerate(vocabulario)}\n",
        "print(char_index)\n",
        "print(index_char)\n",
        "\n",
        "codigos = [char_index[c] for c in texto[:100]]\n",
        "caracteres = [index_char[cod] for cod in codigos]\n",
        "print(f'Primeros 100 caracteres del libro codificados:\\n {codigos}')\n",
        "print(f'Primeros 100 caracteres del libro DEcodificados:\\n {caracteres}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gfGEK5ZmtXd",
        "outputId": "b0e2d331-adb6-479b-ee9b-15ff65e57d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 3 secuencias:\n",
            "\n",
            "'\\ufeffThe Project Gutenberg eBook of Don Quijote, by Miguel de Cervantes Saavedra\\r\\n\\r\\nThis eBook is for the'\n",
            "\n",
            "' use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with a'\n",
            "\n",
            "'lmost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Para entrenar el modelo se tiene que preparar el dataset en secuencias de caracteres\n",
        "# es decir, no le podemos \"enchufar\" todo el libro del tirón, sino que, del mismo modo que\n",
        "# se entrena una red neuronal con una secuencia de diferentes imágenes (entrenamiento supervisado)\n",
        "# en este caso, a partir del libro, generamos secuencias de fragmentos del libro.\n",
        "# además deberán ser secuencias de los caracteres ya codificados...\n",
        "\n",
        "libro_codificado = [char_index[c] for c in texto]\n",
        "\n",
        "seq_length = 100 # tamaño de las secuencias\n",
        "\n",
        "# este es el dataset anterior pero adaptado al formato \"Keras/TensorFlow\"\n",
        "cod_dataset = tf.data.Dataset.from_tensor_slices(libro_codificado)\n",
        "\n",
        "secuencias = cod_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Se implementan las secuencias de seq_length+1 para que, se tome como entrada\n",
        "# el trozo de (0..seq_length) y como salida de (1 .. seq_length+1)\n",
        "\n",
        "print('Primeras 3 secuencias:\\n')\n",
        "for seq in secuencias.take(3):\n",
        "  # seq.numpy() --> convierte tensor en array\n",
        "  cad = ''.join([index_char[cod] for cod in seq.numpy()])\n",
        "  print(f'{repr(cad)}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vldN4lrAsd5i",
        "outputId": "4576a3a4-5018-4d49-f33d-137836ec14cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n",
            "\n",
            "Input: '\\ufeffThe Project Gutenberg eBook of Don Quijote, by Miguel de Cervantes Saavedra\\r\\n\\r\\nThis eBook is for th'\n",
            "Output: 'The Project Gutenberg eBook of Don Quijote, by Miguel de Cervantes Saavedra\\r\\n\\r\\nThis eBook is for the'\n"
          ]
        }
      ],
      "source": [
        "# Ahora, que ya sabemos que tenemos las secuencias y que podemos codificar y\n",
        "# decodificar según nuestros intereses, generamos el dataset\n",
        "\n",
        "def generar_fila_dastaset(fila):\n",
        "    input = fila[:-1]\n",
        "    output = fila[1:]\n",
        "    return input, output\n",
        "\n",
        "# El dataset se almacena en formato \"Keras/Tensorflow\" de modo que se procese\n",
        "# correctamente por nuestra librería durante el entrenamiento, y consta de un\n",
        "# conjunto de secuencias de texto, de modo que el output elimina un caracter de\n",
        "# la izquierda del input y agrega un caracter nuevo a la derecha\n",
        "dataset = secuencias.map(generar_fila_dastaset)\n",
        "print(f'{dataset}\\n')\n",
        "\n",
        "# Veamos una muestra cualquiera de entrada y salida\n",
        "for input, output in dataset.take(1):\n",
        "    entrada = ''.join([index_char[cod] for cod in input.numpy()])\n",
        "    salida = ''.join([index_char[cod] for cod in output.numpy()])\n",
        "    print(f'Input: {repr(entrada)}\\nOutput: {repr(salida)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWv-6YuIvNJ_",
        "outputId": "9d98e0f8-9318-4267-99bb-2f9c7deed5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# Ahora se preparan los hiperparámetros fundamentales para entrenar nuestra red\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# barajamos el dataset y lo organizamos en batches de, en este caso, 64 muestras\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdvZ0iDhwRNU",
        "outputId": "2fc506ee-f394-4aed-b73e-c1209ec08c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           27136     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 106)           108650    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,382,762\n",
            "Trainable params: 5,382,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Construcción del modelo\n",
        "\n",
        "# Nuestro modelo consta de. únicamente, tres capas (en cualquiera de ellas, pero\n",
        "# especialmente en la capa LSTM pùedes observar algunos parámetros con los que\n",
        "# jugar, como el inicializador recurrente).Te recomiento que profundices, no\n",
        "# solo con el libro de Chollet, sino también con la referencia de Keras para\n",
        "# comprender los diferentes hiperparámetros de las capas LSTM y que hagas\n",
        "# pruebas con ellos\n",
        "def crear_modelo(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential()\n",
        "  # La primera capa genera, a partir del vocabulario, un embedding, por lo que\n",
        "  # mapeará, cada carácter de entrada en un vector\n",
        "  model.add(Embedding(input_dim=vocab_size,\n",
        "                      output_dim=embedding_dim,\n",
        "                      batch_input_shape=[batch_size, None]))\n",
        "\n",
        "  # La segunda capa es LSTM, que le da el comportamiento recurrente a\n",
        "  # nuestro modelo. Revisa los argumentos en la documentación de Keras,\n",
        "  # modifica alguno de ellos y ve observando cambios en el resultado final\n",
        "  model.add(LSTM(units=rnn_units, return_sequences=True, stateful=True,\n",
        "                 recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  # Finalmente incorporaremos una capa Densa. No vamos a detenernos a\n",
        "  # explicar este concepto, ya que, además, es una de las capas más\n",
        "  # sencillas. En la bibliografía fundamental (Chollet o Torres,\n",
        "  # por ejemplo) encontrarás, si lo necesitas, información suficiente.\n",
        "  # al no tener una función de activación, retornará un vector con un\n",
        "  # indicador de probabilidad para cada carácter\n",
        "  model.add(Dense(units=vocab_size))\n",
        "\n",
        "  return model\n",
        "\n",
        "vocab_size = len(vocabulario)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "modelo = crear_modelo(vocab_size=vocab_size, embedding_dim=embedding_dim,\n",
        "                      rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "modelo.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z57BvWmX1N3L",
        "outputId": "888db9b9-ab6e-42f5-c967-de626ec3660f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred: (64, 100, 106) (batch_size, sequence_length, vocab_size)\n",
            "[ 15 104  83  73  71  85 102  35  79  51  91  76 103  16  78  38  92   7\n",
            "  76   4  39  27  73   0  84  11  60  59  90  66  17  97  61  54  59   8\n",
            "  40   3  44  86  67  62  75  17  14  81  62  20  82  12  49  69  22  78\n",
            "  24  38  89  71  74  50  10  45  21  15  47  99  27  63  31  95  30  52\n",
            "  47  67  18  87  87  78  12  32  37  44  29  39   4  11  95  98  31  19\n",
            "  78  98  27  46  70  59  59  14   7  96]\n"
          ]
        }
      ],
      "source": [
        "# Se puede observar cómo nuestro modelo devuelve un tensor de salida, con una\n",
        "# dimensión adicional, que es la verosimilitud (no probabilidad, que se\n",
        "# obtendría en todo caso si aplicáramos softmax como función de activación\n",
        "# de la capa densa) para cada uno de los caracteres del vocabulario\n",
        "\n",
        "for input, output in dataset.take(1):\n",
        "  pred = modelo(input)\n",
        "  print(f'Pred: {pred.shape} (batch_size, sequence_length, vocab_size)')\n",
        "\n",
        "# Para oredecir el carácter siguiente, el lector podría pensar en emplear el\n",
        "# carácter más probable, mediante argmax(), pero en lugar de ello, y para\n",
        "# prevenir un posible error obtendremos una muestra probable a partir de la\n",
        "# distribución de salida.\n",
        "\n",
        "# se obtiene una muestra aleatoria de una distribución categórica\n",
        "indices = tf.random.categorical(pred[0], num_samples=1)\n",
        "\n",
        "# squeeze elimina la dimensión adicional de un vector de tamaño 1\n",
        "cars = tf.squeeze(indices, axis=-1).numpy()\n",
        "print(cars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K2vPJ2lq3ftg"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo creado\n",
        "\n",
        "# Claramente se trata de un modelo clasificador, que va a predecir el carácter\n",
        "# que, más probablemente, sigue a partir de la entrada.\n",
        "\n",
        "# Para entrenar el modelo debemos definir una función de pérdida -loss-\n",
        "# y un optimizador.\n",
        "\n",
        "def perdida(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                         logits,\n",
        "                                                         from_logits=True)\n",
        "\n",
        "modelo.compile(optimizer='adam', loss=perdida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KYQjlyp5JGV",
        "outputId": "482f33e0-260c-4575-9d83-1ab37a94bfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "335/335 [==============================] - 34s 84ms/step - loss: 2.1771\n",
            "Epoch 2/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 1.5511\n",
            "Epoch 3/50\n",
            "335/335 [==============================] - 29s 77ms/step - loss: 1.3700\n",
            "Epoch 4/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 1.2888\n",
            "Epoch 5/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 1.2366\n",
            "Epoch 6/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 1.1982\n",
            "Epoch 7/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 1.1668\n",
            "Epoch 8/50\n",
            "335/335 [==============================] - 28s 78ms/step - loss: 1.1382\n",
            "Epoch 9/50\n",
            "335/335 [==============================] - 29s 78ms/step - loss: 1.1113\n",
            "Epoch 10/50\n",
            "335/335 [==============================] - 27s 76ms/step - loss: 1.0863\n",
            "Epoch 11/50\n",
            "335/335 [==============================] - 27s 76ms/step - loss: 1.0609\n",
            "Epoch 12/50\n",
            "335/335 [==============================] - 29s 80ms/step - loss: 1.0361\n",
            "Epoch 13/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 1.0124\n",
            "Epoch 14/50\n",
            "335/335 [==============================] - 27s 76ms/step - loss: 0.9876\n",
            "Epoch 15/50\n",
            "335/335 [==============================] - 30s 79ms/step - loss: 0.9637\n",
            "Epoch 16/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.9399\n",
            "Epoch 17/50\n",
            "335/335 [==============================] - 28s 78ms/step - loss: 0.9158\n",
            "Epoch 18/50\n",
            "335/335 [==============================] - 29s 78ms/step - loss: 0.8934\n",
            "Epoch 19/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 0.8715\n",
            "Epoch 20/50\n",
            "335/335 [==============================] - 29s 80ms/step - loss: 0.8507\n",
            "Epoch 21/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 0.8309\n",
            "Epoch 22/50\n",
            "335/335 [==============================] - 30s 79ms/step - loss: 0.8131\n",
            "Epoch 23/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.7961\n",
            "Epoch 24/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 0.7801\n",
            "Epoch 25/50\n",
            "335/335 [==============================] - 29s 77ms/step - loss: 0.7660\n",
            "Epoch 26/50\n",
            "335/335 [==============================] - 28s 78ms/step - loss: 0.7526\n",
            "Epoch 27/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.7401\n",
            "Epoch 28/50\n",
            "335/335 [==============================] - 29s 78ms/step - loss: 0.7278\n",
            "Epoch 29/50\n",
            "335/335 [==============================] - 28s 78ms/step - loss: 0.7179\n",
            "Epoch 30/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.7088\n",
            "Epoch 31/50\n",
            "335/335 [==============================] - 27s 76ms/step - loss: 0.6997\n",
            "Epoch 32/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.6905\n",
            "Epoch 33/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.6843\n",
            "Epoch 34/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6774\n",
            "Epoch 35/50\n",
            "335/335 [==============================] - 29s 79ms/step - loss: 0.6722\n",
            "Epoch 36/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.6663\n",
            "Epoch 37/50\n",
            "335/335 [==============================] - 28s 78ms/step - loss: 0.6610\n",
            "Epoch 38/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 0.6564\n",
            "Epoch 39/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.6512\n",
            "Epoch 40/50\n",
            "335/335 [==============================] - 28s 79ms/step - loss: 0.6478\n",
            "Epoch 41/50\n",
            "335/335 [==============================] - 27s 75ms/step - loss: 0.6441\n",
            "Epoch 42/50\n",
            "335/335 [==============================] - 30s 80ms/step - loss: 0.6407\n",
            "Epoch 43/50\n",
            "335/335 [==============================] - 27s 76ms/step - loss: 0.6374\n",
            "Epoch 44/50\n",
            "335/335 [==============================] - 28s 79ms/step - loss: 0.6353\n",
            "Epoch 45/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6323\n",
            "Epoch 46/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6291\n",
            "Epoch 47/50\n",
            "335/335 [==============================] - 28s 77ms/step - loss: 0.6267\n",
            "Epoch 48/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6259\n",
            "Epoch 49/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6227\n",
            "Epoch 50/50\n",
            "335/335 [==============================] - 28s 76ms/step - loss: 0.6224\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento del modelo\n",
        "\n",
        "# Debido a que es un entrenamiento largo, y para ilustrar eluso de los\n",
        "# checkpoints (que pueden emplearse para realizar predicciones, por cierto),\n",
        "# vamos a utilizar checkpoints con Keras durante este entrenamiento.\n",
        "\n",
        "# Se recomienda al lector que se documente, también, al respecto.\n",
        "\n",
        "dir_checkpnt = './train_LSTM_OBS_Checkpoints'\n",
        "check_pnt_prefix = os.path.join(dir_checkpnt, 'chkpt_{epoch}')\n",
        "\n",
        "chkpnt_cb = tf.keras.callbacks.ModelCheckpoint(filepath=check_pnt_prefix,\n",
        "                                               save_weights_only=True)\n",
        "\n",
        "EPOCHS=50\n",
        "history = modelo.fit(dataset, epochs=EPOCHS, callbacks=[chkpnt_cb])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TzRRKHVtDMyl"
      },
      "outputs": [],
      "source": [
        "# Generación de texto con el modelo entrenado.\n",
        "\n",
        "# Se van a restaurar los pesos desde el último checkpoint, y además, para\n",
        "# simplificar la generación se va a mantener un batch_size de 1 en la predicción\n",
        "\n",
        "modelo_ok = crear_modelo(vocab_size=vocab_size, embedding_dim=embedding_dim,\n",
        "                         rnn_units=rnn_units, batch_size=1)\n",
        "modelo_ok.load_weights(tf.train.latest_checkpoint(checkpoint_dir=dir_checkpnt))\n",
        "\n",
        "# Debido a que el batch_size es diferente, reconstruimos el modelo\n",
        "modelo_ok.build(tf.TensorShape((1, None)))\n",
        "\n",
        "\n",
        "def generar_texto(modelo, cadena_inicio, temperatura):\n",
        "  num_generate=500 # se generarán 500 caracteres\n",
        "  input = [char_index[car] for car in cadena_inicio]\n",
        "  input = tf.expand_dims(input, 0)\n",
        "  texto = []\n",
        "\n",
        "  # indicará cómo de conservador es el modelo en sus predicciones\n",
        "  # se recomienda hacer pruebas con valores más elevados (menos conservador)\n",
        "  # y más bajos (más conservador)\n",
        "  temp = temperatura\n",
        "\n",
        "  modelo.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    preds = modelo(input)\n",
        "    preds = tf.squeeze(preds, 0)\n",
        "    preds = preds / temp\n",
        "    pred_id = tf.random.categorical(preds, num_samples=1)[-1,0].numpy()\n",
        "    input = tf.expand_dims([pred_id], 0)\n",
        "    texto.append(index_char[pred_id])\n",
        "\n",
        "  return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDIAcGAYGQ5-",
        "outputId": "801ecf9d-38e4-49fb-b442-eac99be54220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto generado sin demasiada creatividad:\n",
            "-------------------------------------------------------------\n",
            ", y como él se imaginaba que\n",
            "había de hacer en el fragies y se lo dijo el mandamiento de su\n",
            "deseo; que, puesto que los conocían que entre los dos se\n",
            "dice que le he menester para condesa para el juez quién\n",
            "ha sido la suya. En resolución, todos los de la carreta, donde le dejaremos por\n",
            "la barca y voto a nosotros el vestido, por lo menos en más estraña catálantida. El cura los tambobernador mío, que no le quiero dejar de ser\n",
            "tanto como el precio de la jineta un personaje, que todo lo demás pu\n",
            "\n",
            "Texto generado con una palabra que no está en el vocabulario:\n",
            "-------------------------------------------------------------\n",
            " de los Amadís de Gaula, que sería destruido de cabellos\n",
            "andantes, que se lo avisé, con que se lo agradece, y que, sin\n",
            "ser alcanzare la verdad de lo que más le fatigaba era de comer a la ciudad, al entrar de\n",
            "su palafrén, acomodado y contento, y, puesto en pie, y, después de haberla ella\n",
            "hacerlas en cobrar en una redoma del cura, y desde allí a poco le aguardaba un bosque sus razones de don Quijote, el cual estaba\n",
            "diciendo:\n",
            "\n",
            "— Agora quiero decir, Sancho, que tiene por costumbre de ver que \n",
            "\n",
            "Dotemos al generador de mayor creatividad...:\n",
            "-------------------------------------------------------------\n",
            " de los Espejos dieron a ruedores transfírmas y honestos agüeros que tornalmente de los\n",
            "libros de caballerías, atómico y fuele de lleno infinito de serlo puñada y anotómete; porque si la cenarlo\n",
            "mismo, que tengo góner con una venta, que le sucedió a don Quijote con una caña de maestresala, y, sobre\n",
            "el otro, echados y revolviéndose a su cargo el escrucientos caballeros, que\n",
            "brecientos iba de acuste y andamos, que no me harán\n",
            "sartísimos principales comenzó a comer, no toparé a Dulcinea que no\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de generación\n",
        "\n",
        "print('Texto generado sin demasiada creatividad:')\n",
        "print('-------------------------------------------------------------')\n",
        "texto = generar_texto(modelo_ok, cadena_inicio='Rocinante', temperatura=0.3)\n",
        "print(''.join(car for car in texto))\n",
        "print()\n",
        "\n",
        "\n",
        "print('Texto generado con una palabra que no está en el vocabulario:')\n",
        "print('-------------------------------------------------------------')\n",
        "texto = generar_texto(modelo_ok, cadena_inicio='red neuronal', temperatura=0.3)\n",
        "print(''.join(car for car in texto))\n",
        "print()\n",
        "\n",
        "\n",
        "print('Dotemos al generador de mayor creatividad...:')\n",
        "print('-------------------------------------------------------------')\n",
        "texto = generar_texto(modelo_ok, cadena_inicio='escudero', temperatura=1)\n",
        "print(''.join(car for car in texto))\n",
        "print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxgHEegg3/o3oIAfzssr29"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}